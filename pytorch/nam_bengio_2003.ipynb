{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the paper\n",
    "# A Neural Probabilistic Language Model Bengio et al, Journal of Machine Learning Research, 2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN architecture](neural_prob_model_pic_1.png \"Image Credit Figure 1 in paper\")  \n",
    "\n",
    "Image credit - Figure 1 from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPM(nn.Module):\n",
    "    def __init__(self, architecture_params):\n",
    "        '''\n",
    "        architecture_params is a dictionary containing all parameters for defining the architecture - \n",
    "        Vocab size V, embedding size m, no of hidden units in layer 1 h and no of previous words to be used as an input to predict\n",
    "        current word n\n",
    "        \n",
    "        '''\n",
    "        super(NPM,self).__init__()\n",
    "        self.V = architecture_params['V'] ## V is the vocab size\n",
    "        self.m = architecture_params['m']  ## m is the embedding size desired\n",
    "        self.n = architecture_params['n'] ## No of prev words used as input to predict current word\n",
    "        self.h = architecture_params['h'] ##  no of hidden units in layer 1\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.V, self.m)  ##  to create an embedding representation where each word in vocab\n",
    "                                                       ## is reprented by a vector of dimension m\n",
    "        self.f1 = nn.Linear(self.n*self.m, self.h) ## n prev words , each of dimension m flattened to create n*m inputs, output of this layer is hidden size h\n",
    "        self.f2 = nn.Linear(self.h, self.V)  ## output is of size of vocab as we are predicting next word\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        assumes a batched input\n",
    "        input : tensor of dimension batch_size * \n",
    "        output : tensor of probabilities :  dimension batch_size * V\n",
    "        '''\n",
    "        batch_size = inputs.shape[0]\n",
    "        embeds = self.embedding(inputs).reshape(batch_size,1,-1) ## to flatten - before reshaping, (after embedding) of  dimension batch_size * no of words * embedding size; after flattening, of dimension batch_size * (no of words * embedding size)\n",
    "        h1 = torch.tanh(self.f1(embeds))  ## The tanh transformation in the first hidden layer\n",
    "        outp = torch.log_softmax(self.f2(h1), dim=-1) ## final output - use log_softmax which is  log of softmax as implementation is more stable and efficient\n",
    "                                                    ## dim=-1 because we want softmax applied across the last dimension of the tensor\n",
    "        return outp\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining architecture parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 2 ## use 2 words before to predict current word\n",
    "num_hidden_units = 5  ## in the hidden layer\n",
    "embedding_size = 10  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Convert text to tuple of input words and target for LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tuples_for_embedding(text_list, use_both_sides = False, CONTEXT_SIZE = 5):\n",
    "    '''Aim : generate list of tuples of form ([xi-k,xi-(k-1)]...xi-1],xi) . Used as input for different NN models'''\n",
    "    list_of_tup = []\n",
    "    for text in text_list:\n",
    "        text = text.split()\n",
    "        \n",
    "        if use_both_sides :\n",
    "            start_index_target_word = CONTEXT_SIZE\n",
    "            end_index_target_word = len(text)-CONTEXT_SIZE      \n",
    "\n",
    "        else: ## use only words before context word\n",
    "            start_index_target_word = CONTEXT_SIZE\n",
    "            end_index_target_word = len(text)\n",
    "        for i in range(start_index_target_word,end_index_target_word):\n",
    "            feature_list = []\n",
    "            if use_both_sides :\n",
    "                max_response_index = i + CONTEXT_SIZE\n",
    "            else:\n",
    "                max_response_index = i\n",
    "\n",
    "\n",
    "            for j in range( (i-CONTEXT_SIZE),max_response_index):\n",
    "                if j==i:\n",
    "                    pass\n",
    "                else:\n",
    "                    feature_list.append(text[j])\n",
    "            list_of_tup.append((feature_list,text[i]))\n",
    "    return list_of_tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['I', 'went'], 'to'),\n",
       " (['went', 'to'], 'school'),\n",
       " (['to', 'school'], 'yesterday')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_tuples_for_embedding([\"I went to school yesterday\"], False, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get w2index and index2word from input corpus (list of sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2index_index2word(sentence_list):\n",
    "    '''Given a list of sentences, create word to index and index to word from it'''\n",
    "    list_of_words =  \" \".join(sentences).split()\n",
    "\n",
    "    list_of_words = list(set(list_of_words))\n",
    "    word_to_index_dict = {w : i for i,w in enumerate(list_of_words)}\n",
    "    index_to_word_dict = {i : w for i,w in enumerate(list_of_words)}\n",
    "    \n",
    "    return word_to_index_dict, index_to_word_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to batch input data, expected as tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(TUP = None, batch_size = None):\n",
    "    curr_index = 0\n",
    "    print(curr_index)\n",
    "    while curr_index <= max(0,len(TUP)-batch_size):\n",
    "        last_index = min(curr_index + batch_size,len(TUP))        \n",
    "        curr_tup = TUP[curr_index : last_index]\n",
    "        print(curr_tup)\n",
    "        curr_index = curr_index + batch_size\n",
    "        yield curr_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert tuple to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tup_to_tensor(TUP, word_to_index):\n",
    "    context_list = []\n",
    "    response_list = []\n",
    "    for tup in TUP:\n",
    "        curr_context = tup[0]\n",
    "        curr_response = tup[1]\n",
    "        context_as_index = [word_to_index[word] for word in curr_context]\n",
    "        context_as_index = torch.tensor(context_as_index,dtype = torch.long)\n",
    "        target_as_index = torch.tensor([word_to_index[curr_response]],dtype = torch.long)\n",
    "        \n",
    "        context_list.append(context_as_index)\n",
    "        response_list.append(target_as_index)\n",
    "    return torch.stack(context_list), torch.stack(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I went to school today\", \"I will not go to the shop tomorrow\", \"Hi, How are you ? \"]\n",
    "word_to_index_dict, index_to_word_dict = get_w2index_index2word(sentences)\n",
    "lm_tuples = generate_tuples_for_embedding(text_list=sentences, use_both_sides=False, CONTEXT_SIZE=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['I', 'went'], 'to'),\n",
       " (['went', 'to'], 'school'),\n",
       " (['to', 'school'], 'today'),\n",
       " (['I', 'will'], 'not'),\n",
       " (['will', 'not'], 'go'),\n",
       " (['not', 'go'], 'to'),\n",
       " (['go', 'to'], 'the'),\n",
       " (['to', 'the'], 'shop'),\n",
       " (['the', 'shop'], 'tomorrow'),\n",
       " (['Hi,', 'How'], 'are'),\n",
       " (['How', 'are'], 'you'),\n",
       " (['are', 'you'], '?')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor = convert_tup_to_tensor(lm_tuples, word_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13],\n",
       "        [ 4],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 6],\n",
       "        [13],\n",
       "        [ 0],\n",
       "        [11],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [ 5],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index_dict.keys())\n",
    "architecture_params = {}\n",
    "architecture_params['V'] = vocab_size\n",
    "architecture_params['n'] = context_length\n",
    "architecture_params['m'] = embedding_size\n",
    "architecture_params['h'] = num_hidden_units\n",
    "##\n",
    "model = NPM(architecture_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V': 16, 'n': 2, 'm': 10, 'h': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NPM(\n",
       "  (embedding): Embedding(16, 10)\n",
       "  (f1): Linear(in_features=20, out_features=5, bias=True)\n",
       "  (f2): Linear(in_features=5, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "def train():\n",
    "    all_losses = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        print(epoch)\n",
    "        epoch_loss = 0\n",
    "        for tup in generate_batch(TUP=lm_tuples, batch_size=batch_size):\n",
    "            context_as_tensor_batch, target_as_tensor_batch = convert_tup_to_tensor(tup, word_to_index_dict)\n",
    "\n",
    "            model.zero_grad()\n",
    "            outp_prob = model(context_as_tensor_batch)\n",
    "            outp_prob_reshape = outp_prob.reshape(len(outp_prob),outp_prob.shape[2])\n",
    "            target_as_tensor_batch_reshape = target_as_tensor_batch.reshape(len(target_as_tensor_batch))\n",
    "            curr_loss = loss(outp_prob_reshape,target_as_tensor_batch_reshape)\n",
    "            print('%%%% curr loss %f'%(curr_loss.item()))\n",
    "            epoch_loss = epoch_loss + curr_loss.item()\n",
    "            curr_loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch %d : loss %f'%(epoch,epoch_loss))\n",
    "    all_losses.append(epoch_loss)\n",
    "    print(all_losses)\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "[(['I', 'went'], 'to'), (['went', 'to'], 'school'), (['to', 'school'], 'today'), (['I', 'will'], 'not'), (['will', 'not'], 'go'), (['not', 'go'], 'to'), (['go', 'to'], 'the'), (['to', 'the'], 'shop'), (['the', 'shop'], 'tomorrow'), (['Hi,', 'How'], 'are'), (['How', 'are'], 'you'), (['are', 'you'], '?')]\n",
      "tensor([[ 3,  9],\n",
      "        [ 9, 13],\n",
      "        [13,  4],\n",
      "        [ 3, 12],\n",
      "        [12,  8],\n",
      "        [ 8,  6],\n",
      "        [ 6, 13],\n",
      "        [13,  0],\n",
      "        [ 0, 11],\n",
      "        [14, 15],\n",
      "        [15,  2],\n",
      "        [ 2,  5]]) tensor([[13],\n",
      "        [ 4],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 6],\n",
      "        [13],\n",
      "        [ 0],\n",
      "        [11],\n",
      "        [10],\n",
      "        [ 2],\n",
      "        [ 5],\n",
      "        [ 1]])\n",
      "hi\n",
      "tensor([[-2.1360, -2.5079, -2.9242, -2.9128, -2.6829, -3.2109, -2.2670, -3.0448,\n",
      "         -2.8601, -3.0864, -2.7498, -3.3964, -2.9572, -3.2118, -2.3863, -3.0384],\n",
      "        [-2.2113, -2.8212, -2.5103, -2.9681, -2.6770, -2.7890, -2.7073, -2.8605,\n",
      "         -3.0057, -2.8274, -2.8515, -2.8250, -3.0172, -3.0637, -2.5929, -3.0391],\n",
      "        [-2.4978, -2.7584, -2.8297, -2.7637, -2.8948, -3.2926, -2.6212, -3.2572,\n",
      "         -3.3183, -2.7007, -2.8070, -3.0972, -2.8136, -2.6297, -2.1414, -2.6583],\n",
      "        [-2.1064, -2.5832, -2.9794, -2.9513, -2.5628, -3.2009, -2.5258, -3.0519,\n",
      "         -2.8254, -2.9315, -2.8401, -3.2285, -3.1447, -3.1155, -2.2845, -2.9056],\n",
      "        [-2.4697, -2.6415, -2.8863, -2.7308, -2.7157, -3.0038, -2.2956, -3.1464,\n",
      "         -3.1294, -3.0255, -2.7523, -3.3929, -2.7242, -2.8107, -2.2560, -3.1346],\n",
      "        [-2.5288, -2.7910, -2.7096, -2.8201, -2.9377, -2.9970, -2.7484, -3.1208,\n",
      "         -3.3706, -2.6031, -2.8297, -3.0392, -2.6752, -2.6174, -2.2056, -2.9141],\n",
      "        [-2.2222, -2.6304, -2.6382, -2.9574, -2.8987, -2.9973, -2.5465, -2.9306,\n",
      "         -3.0882, -2.7842, -2.7656, -3.1082, -2.7540, -3.0635, -2.4466, -3.0483],\n",
      "        [-2.3989, -2.5977, -2.8280, -2.9184, -3.1500, -3.2921, -2.6788, -3.1450,\n",
      "         -3.3584, -2.5392, -2.7785, -3.2384, -2.5718, -2.7824, -2.1363, -2.8405],\n",
      "        [-2.3567, -2.6998, -2.5098, -2.9812, -3.3072, -3.2320, -2.7418, -3.0298,\n",
      "         -3.4332, -2.4904, -2.7567, -2.9720, -2.5438, -2.8965, -2.3954, -2.7747],\n",
      "        [-2.0421, -2.5141, -2.4968, -3.0910, -3.0963, -3.0995, -2.2969, -2.8126,\n",
      "         -3.0006, -2.9909, -2.6796, -3.2272, -2.6857, -3.5053, -2.7709, -3.1816],\n",
      "        [-2.2076, -2.7690, -2.6195, -2.9231, -2.7559, -3.1247, -2.6571, -3.0072,\n",
      "         -3.0485, -2.7838, -2.8210, -2.8713, -3.0450, -3.0000, -2.4545, -2.7345],\n",
      "        [-3.0277, -3.0980, -2.9323, -2.4013, -2.6129, -3.1692, -2.4283, -3.5954,\n",
      "         -3.5361, -3.0959, -2.8695, -3.1549, -2.9332, -2.1666, -2.1032, -2.6885]],\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "%% curr loss 2.818425\n",
      "epoch 0 : loss 2.818425\n",
      "[2.818425178527832]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
